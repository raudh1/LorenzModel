{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import os\n",
    "import sys\n",
    "#from torch import Dataset, Dataloader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "dirName = './RESULTS'\n",
    "if not os.path.exists(dirName):\n",
    "    os.makedirs(dirName)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self, hidden, layer, features, dropout):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.hidden   = hidden\n",
    "        self.layer    = layer\n",
    "        self.features = features\n",
    "\n",
    "        self.lstm1  = nn.LSTM(self.features, self.hidden, self.layer, dropout=dropout)\n",
    "        self.linear = nn.Linear(self.hidden, self.features)\n",
    " \n",
    "    def forward(self, input, h_t, c_t):\n",
    "        self.lstm1.flatten_parameters()\n",
    "        out, (h_t, c_t) = self.lstm1(input, (h_t, c_t))\n",
    "        output = out.view(input.size(0)*input.size(1),self.hidden)\n",
    "        output = self.linear(output)\n",
    "        output = output.view(input.size(0),input.size(1),self.features)\n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (100, 1, 3)\n",
      "FILE:  0 STEP TEST:  0 test loss: 274.896037366899 lr:  0.01 err:  27.3896037366899\n",
      "FILE:  0 STEP TEST:  10 test loss: 163.58582999362304 lr:  0.01 err:  11.131020737327598\n",
      "FILE:  0 STEP TEST:  20 test loss: 105.21877756578002 lr:  0.01 err:  5.836705242784302\n",
      "FILE:  0 STEP TEST:  30 test loss: 66.15309715386644 lr:  0.01 err:  3.9065680411913575\n",
      "FILE:  0 STEP TEST:  40 test loss: 45.102490104502934 lr:  0.01 err:  2.1050607049363506\n",
      "FILE:  0 STEP TEST:  50 test loss: 35.44394389403481 lr:  0.01 err:  0.9658546210468124\n",
      "FILE:  0 STEP TEST:  60 test loss: 30.826284587666823 lr:  0.01 err:  0.46176593063679866\n",
      "FILE:  0 STEP TEST:  70 test loss: 28.485236887922614 lr:  0.01 err:  0.2341047699744209\n",
      "FILE:  0 STEP TEST:  80 test loss: 26.897238310693414 lr:  0.01 err:  0.15879985772292002\n",
      "FILE:  0 STEP TEST:  90 test loss: 25.712257605779527 lr:  0.01 err:  0.11849807049138868\n",
      "FILE:  0 STEP TEST:  100 test loss: 24.664736794523375 lr:  0.01 err:  0.10475208112561525\n",
      "FILE:  0 STEP TEST:  110 test loss: 23.758309265493818 lr:  0.01 err:  0.09064275290295569\n",
      "FILE:  0 STEP TEST:  120 test loss: 22.989386531617196 lr:  0.01 err:  0.07689227338766216\n",
      "FILE:  0 STEP TEST:  130 test loss: 22.307193467776152 lr:  0.01 err:  0.06821930638410442\n",
      "FILE:  0 STEP TEST:  140 test loss: 21.689758391861226 lr:  0.01 err:  0.06174350759149263\n",
      "FILE:  0 STEP TEST:  150 test loss: 21.160253555586717 lr:  0.01 err:  0.05295048362745085\n",
      "FILE:  0 STEP TEST:  160 test loss: 25.398704412730957 lr:  0.01 err:  0.42384508571442403\n",
      "FILE:  0 STEP TEST:  170 test loss: 24.887233769133832 lr:  0.01 err:  0.0511470643597125\n",
      "FILE:  0 STEP TEST:  180 test loss: 24.188303763434394 lr:  0.01 err:  0.06989300056994381\n",
      "FILE:  0 STEP TEST:  190 test loss: 23.819706236897254 lr:  0.01 err:  0.03685975265371404\n",
      "END: 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    # load data and make training set\n",
    "    hidden   = 50\n",
    "    layer    = 2\n",
    "    features = 3\n",
    "    dropout  = 0\n",
    "    # build the model\n",
    "    case = 'FULL'\n",
    "        \n",
    "    ini = 25\n",
    "    train_loss= []\n",
    "    train_loss_grad= []\n",
    "    for k in [0] :\n",
    "        name = './DATA/TRAIN_npy/'+case+'_'+str(k)+'.npy'\n",
    "        data = np.load(name)\n",
    "        data = np.expand_dims(data.T, axis=1)\n",
    "        data = data[:100] # reduce the number of time steps (db)\n",
    "        print(\"Data shape: \", data.shape)\n",
    "        input  = torch.from_numpy(data[:-1,:,:]).double().to(device)\n",
    "        target = torch.from_numpy(data[1:,:,:]).double().to(device)\n",
    "\n",
    "        seq = Sequence(hidden,layer,features,dropout).double().to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(seq.parameters(), lr =0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.7, patience=50, min_lr = 5e-5)\n",
    "        dirName = './RESULTS/'+case+'_'+str(k)\n",
    "        if not os.path.exists(dirName):\n",
    "            os.makedirs(dirName)\n",
    "        h_0 = torch.normal(mean=0.0, std=torch.ones(layer,input.size(1), hidden, dtype=torch.double)).to(device)\n",
    "        c_0 = torch.normal(mean=0.0, std=torch.ones(layer,input.size(1), hidden, dtype=torch.double)).to(device)\n",
    "\n",
    "        if k==0:\n",
    "            torch.save(seq.state_dict(),'./init_model.pt')\n",
    "        else:\n",
    "            state_dict = torch.load('./init_model.pt', map_location=device)\n",
    "            seq.load_state_dict(state_dict)\n",
    "\n",
    "        err = 10\n",
    "        loss1 = 1\n",
    "        i = 0\n",
    "        while loss1 > 1e-4 and i<200:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input,h_0,c_0)\n",
    "            loss = criterion(out, target.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            train_loss.append(loss.item())\n",
    "            #train_loss_grad.append()\n",
    "            torch.save(seq.state_dict(),dirName+'/mytraining.pt')\n",
    "            np.savetxt(dirName+'/loss.out',np.array([loss1]))\n",
    "            if (i%(10)==0):\n",
    "                loss2 = loss\n",
    "                err = np.abs(loss2.item()-loss1)/10\n",
    "                loss1 = loss2.item()\n",
    "                print('FILE: ', k,'STEP TEST: ', i, 'test loss:', loss2.item(), 'lr: ', optimizer.param_groups[0]['lr'], 'err: ', err)\n",
    "            i += 1\n",
    "            sys.stdout.flush() \n",
    "        print('END: '+str(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': [Parameter containing:\n",
       "  tensor([[-0.1051,  0.0461,  0.0972],\n",
       "          [ 0.0266,  0.0331,  0.1372],\n",
       "          [-0.0536,  0.1362,  0.0806],\n",
       "          [ 0.0011, -0.0264, -0.1683],\n",
       "          [ 0.0453,  0.0889,  0.1101],\n",
       "          [ 0.0373,  0.1463,  0.2022],\n",
       "          [ 0.0407, -0.1681, -0.1805],\n",
       "          [-0.0230,  0.1704,  0.0933],\n",
       "          [ 0.0358, -0.0634, -0.1613],\n",
       "          [ 0.0313,  0.2110,  0.1102],\n",
       "          [ 0.1723,  0.2067,  0.0348],\n",
       "          [ 0.0910,  0.1237,  0.2059],\n",
       "          [-0.0113, -0.0415,  0.1401],\n",
       "          [-0.0009, -0.0108,  0.1577],\n",
       "          [-0.0528, -0.0533,  0.0977],\n",
       "          [ 0.0874,  0.2201,  0.0678],\n",
       "          [ 0.1541,  0.1001,  0.0040],\n",
       "          [-0.0257, -0.0331,  0.1875],\n",
       "          [-0.0295,  0.1923,  0.0986],\n",
       "          [-0.2453, -0.0874,  0.0129],\n",
       "          [-0.1868, -0.0742,  0.1262],\n",
       "          [-0.1441,  0.0448,  0.0712],\n",
       "          [ 0.1947, -0.0247,  0.1920],\n",
       "          [ 0.0357,  0.2140,  0.1224],\n",
       "          [ 0.0616,  0.1788,  0.1985],\n",
       "          [-0.0749, -0.1314, -0.1939],\n",
       "          [ 0.0839, -0.0079,  0.0619],\n",
       "          [-0.1430,  0.0069,  0.0875],\n",
       "          [ 0.0013,  0.2280,  0.1682],\n",
       "          [ 0.0707,  0.2226,  0.0450],\n",
       "          [ 0.1245,  0.1816,  0.1296],\n",
       "          [ 0.1640,  0.0657,  0.2198],\n",
       "          [-0.0508,  0.1835, -0.0195],\n",
       "          [ 0.0849,  0.2092,  0.1231],\n",
       "          [ 0.1755,  0.0052,  0.0435],\n",
       "          [ 0.1994,  0.1602,  0.0775],\n",
       "          [ 0.0444,  0.1944,  0.0143],\n",
       "          [ 0.1123,  0.0995,  0.1288],\n",
       "          [ 0.0747, -0.1587,  0.2264],\n",
       "          [ 0.0643, -0.1867, -0.1274],\n",
       "          [-0.0148,  0.0995,  0.1533],\n",
       "          [-0.0414,  0.0113,  0.1947],\n",
       "          [ 0.0408,  0.1226,  0.1540],\n",
       "          [ 0.1859,  0.2022,  0.0782],\n",
       "          [ 0.0859,  0.0041, -0.0178],\n",
       "          [-0.0442,  0.1687,  0.1908],\n",
       "          [ 0.0173,  0.0520,  0.1206],\n",
       "          [-0.0004,  0.0193,  0.1163],\n",
       "          [-0.0476,  0.1268, -0.0264],\n",
       "          [ 0.1253,  0.0102,  0.1968],\n",
       "          [ 0.0525, -0.0601,  0.2138],\n",
       "          [ 0.1078,  0.0833,  0.0151],\n",
       "          [ 0.2456,  0.2317, -0.0181],\n",
       "          [ 0.0730, -0.1696, -0.1862],\n",
       "          [ 0.1973,  0.0362,  0.0615],\n",
       "          [ 0.0187,  0.0250,  0.0797],\n",
       "          [-0.0306, -0.0266, -0.0012],\n",
       "          [-0.0231,  0.0431,  0.1557],\n",
       "          [-0.0891, -0.0696, -0.0189],\n",
       "          [ 0.0796,  0.0435,  0.0673],\n",
       "          [-0.1784,  0.0718,  0.0632],\n",
       "          [ 0.2392,  0.0731,  0.0151],\n",
       "          [ 0.0684,  0.1436,  0.0929],\n",
       "          [ 0.2544, -0.0180,  0.0489],\n",
       "          [-0.2180,  0.0704,  0.1825],\n",
       "          [ 0.1264,  0.1283,  0.0402],\n",
       "          [ 0.2238,  0.0674,  0.0652],\n",
       "          [-0.0463,  0.1486,  0.0207],\n",
       "          [ 0.0286,  0.0339,  0.1050],\n",
       "          [ 0.2030,  0.1933,  0.1464],\n",
       "          [-0.0850,  0.0223,  0.1659],\n",
       "          [ 0.0331,  0.0827,  0.2131],\n",
       "          [ 0.0587,  0.0478,  0.1249],\n",
       "          [ 0.0054, -0.0027,  0.1446],\n",
       "          [ 0.1896,  0.0690,  0.0369],\n",
       "          [ 0.0601, -0.0383, -0.1648],\n",
       "          [ 0.0924,  0.1489,  0.1472],\n",
       "          [-0.1027, -0.0181,  0.0172],\n",
       "          [ 0.1632,  0.1322,  0.0603],\n",
       "          [ 0.0876,  0.1709, -0.0323],\n",
       "          [ 0.0272,  0.1819,  0.0805],\n",
       "          [ 0.0496,  0.1857, -0.0043],\n",
       "          [-0.0268,  0.0032,  0.2298],\n",
       "          [ 0.0291,  0.1978,  0.0754],\n",
       "          [-0.0612,  0.1013,  0.0049],\n",
       "          [ 0.0170,  0.2075,  0.0008],\n",
       "          [ 0.1667,  0.1472,  0.0743],\n",
       "          [-0.0985,  0.1358, -0.1252],\n",
       "          [-0.1489,  0.1311,  0.0346],\n",
       "          [ 0.0289, -0.1619, -0.1931],\n",
       "          [ 0.0089,  0.0140,  0.1010],\n",
       "          [-0.0503,  0.0332,  0.0712],\n",
       "          [ 0.0716,  0.0631,  0.0776],\n",
       "          [ 0.1865,  0.0943, -0.0384],\n",
       "          [ 0.1758,  0.0768, -0.0317],\n",
       "          [ 0.1935, -0.0040,  0.1247],\n",
       "          [ 0.0395,  0.1833,  0.0822],\n",
       "          [ 0.0071,  0.0733,  0.1039],\n",
       "          [-0.0336,  0.0708,  0.1333],\n",
       "          [ 0.1578,  0.1148,  0.0615],\n",
       "          [ 0.0008,  0.0090,  0.1537],\n",
       "          [-0.0594, -0.0651, -0.1119],\n",
       "          [ 0.1797, -0.0637, -0.1106],\n",
       "          [-0.0977, -0.1811, -0.0138],\n",
       "          [ 0.0079,  0.1984,  0.1375],\n",
       "          [-0.0842, -0.1578, -0.1372],\n",
       "          [-0.0898,  0.0675, -0.0593],\n",
       "          [ 0.0154,  0.0068, -0.1742],\n",
       "          [-0.1738, -0.0064,  0.0560],\n",
       "          [-0.0089,  0.0787,  0.2208],\n",
       "          [ 0.1472,  0.1976,  0.1621],\n",
       "          [ 0.1170,  0.0814,  0.2160],\n",
       "          [-0.1351, -0.0115, -0.1214],\n",
       "          [-0.0684, -0.1784, -0.2519],\n",
       "          [ 0.0324, -0.0495,  0.2028],\n",
       "          [-0.1742, -0.2208, -0.1146],\n",
       "          [ 0.1555,  0.2518,  0.1289],\n",
       "          [-0.0246,  0.0576,  0.1049],\n",
       "          [-0.0650,  0.0372, -0.1956],\n",
       "          [ 0.0678,  0.0129,  0.1228],\n",
       "          [-0.0783, -0.0083, -0.1814],\n",
       "          [-0.0399,  0.0579,  0.1397],\n",
       "          [ 0.1950,  0.0672,  0.2019],\n",
       "          [ 0.1970, -0.0324,  0.1481],\n",
       "          [ 0.1822,  0.1652,  0.2264],\n",
       "          [ 0.0170,  0.0734,  0.0054],\n",
       "          [ 0.0859,  0.2148,  0.1922],\n",
       "          [-0.1446, -0.0224,  0.0470],\n",
       "          [ 0.0850,  0.0959,  0.1304],\n",
       "          [-0.1331, -0.0150, -0.1604],\n",
       "          [ 0.0026,  0.1267,  0.0921],\n",
       "          [-0.1509, -0.0222, -0.2175],\n",
       "          [-0.1250, -0.0782, -0.1778],\n",
       "          [-0.0781, -0.0823, -0.1375],\n",
       "          [-0.0126, -0.1020, -0.1319],\n",
       "          [ 0.1564,  0.1853,  0.2087],\n",
       "          [ 0.2086, -0.0110,  0.2228],\n",
       "          [ 0.0216,  0.1542,  0.1127],\n",
       "          [-0.0090,  0.0056, -0.1176],\n",
       "          [-0.1177, -0.0356, -0.0185],\n",
       "          [-0.1895, -0.0959, -0.0170],\n",
       "          [-0.0464,  0.0697, -0.1399],\n",
       "          [-0.1659,  0.0146, -0.1557],\n",
       "          [-0.2071, -0.1723, -0.0506],\n",
       "          [ 0.1551,  0.0493,  0.1518],\n",
       "          [-0.0216, -0.1941, -0.0908],\n",
       "          [ 0.0490,  0.1977,  0.1988],\n",
       "          [-0.0464,  0.1979,  0.2030],\n",
       "          [-0.0552, -0.1535, -0.0994],\n",
       "          [-0.0320,  0.0293,  0.1807],\n",
       "          [ 0.1607,  0.0268,  0.1193],\n",
       "          [ 0.1742,  0.1524,  0.1694],\n",
       "          [ 0.2022,  0.1420,  0.1104],\n",
       "          [-0.0334,  0.0910, -0.0395],\n",
       "          [ 0.0988,  0.1904,  0.2309],\n",
       "          [-0.0252,  0.1026,  0.1313],\n",
       "          [-0.1465,  0.0255, -0.0700],\n",
       "          [ 0.0797,  0.2063,  0.1415],\n",
       "          [-0.1587, -0.0472,  0.0723],\n",
       "          [ 0.1454,  0.1080,  0.0456],\n",
       "          [ 0.0701,  0.1990,  0.1200],\n",
       "          [-0.0034,  0.1365,  0.0594],\n",
       "          [ 0.2326,  0.1706,  0.0065],\n",
       "          [-0.0431,  0.0227,  0.0871],\n",
       "          [ 0.0023,  0.1422,  0.1790],\n",
       "          [ 0.1392,  0.1648,  0.1326],\n",
       "          [ 0.2363,  0.1381,  0.0523],\n",
       "          [-0.1465,  0.0289,  0.1276],\n",
       "          [-0.0183,  0.0981,  0.2153],\n",
       "          [-0.0569,  0.0378,  0.1684],\n",
       "          [-0.1822,  0.0221,  0.0822],\n",
       "          [ 0.0632,  0.1200,  0.1597],\n",
       "          [ 0.0883,  0.0958,  0.0717],\n",
       "          [ 0.0783,  0.0103,  0.0863],\n",
       "          [ 0.1548,  0.0322,  0.0440],\n",
       "          [-0.0445,  0.0382,  0.0754],\n",
       "          [ 0.0842,  0.1967,  0.1718],\n",
       "          [-0.2126, -0.1050,  0.1533],\n",
       "          [ 0.1754,  0.2720,  0.2477],\n",
       "          [-0.0259,  0.2034,  0.1837],\n",
       "          [ 0.1070,  0.0370,  0.1351],\n",
       "          [ 0.1706,  0.0197,  0.1936],\n",
       "          [-0.0108,  0.0984,  0.1020],\n",
       "          [ 0.1808,  0.0365,  0.1139],\n",
       "          [ 0.0811, -0.1453, -0.1057],\n",
       "          [ 0.0592,  0.2042,  0.1774],\n",
       "          [ 0.0279,  0.0449,  0.1871],\n",
       "          [ 0.0399,  0.2027,  0.1617],\n",
       "          [-0.0477,  0.2216,  0.1554],\n",
       "          [ 0.0420, -0.1301,  0.0464],\n",
       "          [ 0.0828, -0.0602, -0.0914],\n",
       "          [-0.1198, -0.1967,  0.1628],\n",
       "          [ 0.0889,  0.0911,  0.2503],\n",
       "          [ 0.0224,  0.1309,  0.1560],\n",
       "          [-0.0584, -0.0872, -0.1734],\n",
       "          [ 0.0519,  0.2379,  0.2251],\n",
       "          [ 0.0627,  0.0774,  0.1248],\n",
       "          [ 0.0285,  0.1553,  0.1361],\n",
       "          [ 0.1328,  0.0643,  0.0768],\n",
       "          [ 0.1670,  0.2472,  0.0741]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.1822, -0.1341,  0.0219,  ..., -0.0780,  0.1662, -0.1576],\n",
       "          [ 0.1464, -0.0014, -0.2587,  ...,  0.2387, -0.0529,  0.0490],\n",
       "          [ 0.0597,  0.0167,  0.0108,  ...,  0.2126,  0.1917,  0.1394],\n",
       "          ...,\n",
       "          [-0.0149, -0.2281,  0.0216,  ...,  0.1658, -0.1496, -0.0039],\n",
       "          [ 0.3031, -0.1188, -0.1619,  ...,  0.2256, -0.0624,  0.1589],\n",
       "          [ 0.0702, -0.2715, -0.0824,  ...,  0.1258, -0.0898,  0.0016]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0093,  0.0247,  0.0804,  0.0454,  0.1028,  0.0799, -0.0102,  0.1479,\n",
       "          -0.0741,  0.2498,  0.0847,  0.0637, -0.0124,  0.0190,  0.2642,  0.1144,\n",
       "           0.1592,  0.1753,  0.2009,  0.1344,  0.0356,  0.0838,  0.1471,  0.1541,\n",
       "           0.2291, -0.0622,  0.2707,  0.1051,  0.1376,  0.2093, -0.0050,  0.0674,\n",
       "           0.0450,  0.1328,  0.1284,  0.0146,  0.1842,  0.1813,  0.2643, -0.0892,\n",
       "           0.0148, -0.0231,  0.1031,  0.1251, -0.0415, -0.0243,  0.1267,  0.1646,\n",
       "          -0.0691,  0.2337, -0.0054, -0.0135, -0.1329,  0.0580,  0.0430, -0.0180,\n",
       "          -0.0229,  0.0773, -0.1880,  0.1394,  0.3104,  0.2664,  0.1745,  0.0603,\n",
       "           0.0398,  0.0727,  0.2439,  0.1287,  0.0623,  0.1745,  0.1112,  0.0023,\n",
       "           0.0927,  0.1514,  0.2067, -0.1965,  0.0371,  0.1441,  0.0845,  0.2394,\n",
       "           0.0567,  0.1066,  0.1263,  0.0153,  0.1192, -0.0353,  0.1162, -0.1172,\n",
       "           0.1143,  0.0217, -0.1772,  0.1334,  0.0363,  0.1036,  0.0803,  0.1457,\n",
       "           0.0332,  0.1897,  0.0391,  0.2234,  0.0822, -0.1594, -0.1440, -0.1180,\n",
       "           0.0425, -0.0049, -0.1235,  0.0861, -0.0380, -0.0375,  0.1013,  0.1484,\n",
       "          -0.0689, -0.1393, -0.0621, -0.0760, -0.0178, -0.0032,  0.0036,  0.0507,\n",
       "          -0.1608,  0.2301,  0.0132,  0.0964, -0.0240,  0.0076,  0.2282,  0.0590,\n",
       "           0.2376, -0.2238, -0.0237, -0.0818, -0.1078, -0.0780, -0.0811, -0.0005,\n",
       "           0.1433,  0.2069, -0.1759,  0.0953,  0.0531, -0.1947, -0.1397, -0.0430,\n",
       "          -0.0306, -0.0186,  0.0279, -0.0044, -0.0153,  0.1536,  0.0321,  0.0895,\n",
       "           0.1842, -0.1556,  0.0470,  0.1077,  0.0422,  0.0385,  0.0435,  0.1862,\n",
       "           0.2534,  0.2440,  0.2144,  0.0650,  0.1418,  0.1308,  0.0374,  0.0568,\n",
       "           0.2531,  0.1606,  0.0871,  0.2514,  0.1419,  0.1561,  0.2119, -0.1499,\n",
       "           0.0968,  0.1334,  0.0461,  0.1883,  0.1178,  0.0486,  0.0949, -0.0112,\n",
       "          -0.1246,  0.1283,  0.2317,  0.2041,  0.2350, -0.1371, -0.2046,  0.1555,\n",
       "           0.2265,  0.2009, -0.0065,  0.1127,  0.1030,  0.0205,  0.0734,  0.2292],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1797,  0.0981,  0.0386, -0.0746,  0.0120,  0.0111, -0.0751, -0.0138,\n",
       "          -0.0201,  0.2016,  0.0947,  0.1869,  0.0902,  0.1005,  0.0891,  0.1091,\n",
       "           0.1320,  0.0500,  0.0731,  0.0540,  0.1115,  0.2473,  0.1569,  0.1454,\n",
       "           0.1798, -0.0649,  0.0075,  0.1419,  0.1978,  0.1588,  0.0137,  0.1217,\n",
       "           0.1730,  0.0171,  0.0627,  0.0083,  0.0821,  0.0728, -0.0040, -0.0237,\n",
       "           0.1299,  0.1212,  0.0551,  0.1473,  0.1486,  0.0566,  0.0580,  0.0357,\n",
       "          -0.0037,  0.0673,  0.0753,  0.1967, -0.0354,  0.0411,  0.1838,  0.1166,\n",
       "          -0.1821,  0.1380,  0.0225,  0.1464,  0.2832,  0.0263, -0.0406,  0.2080,\n",
       "           0.1987,  0.1138,  0.2438,  0.0747, -0.0105,  0.1653,  0.1439, -0.0229,\n",
       "           0.1645,  0.0524,  0.1888,  0.0565,  0.2526,  0.0288,  0.2452,  0.0230,\n",
       "          -0.0657,  0.0522,  0.1414,  0.2055,  0.0947,  0.1156,  0.1004, -0.0766,\n",
       "           0.2235, -0.0833, -0.1524,  0.0066,  0.2059, -0.0658, -0.0414,  0.2293,\n",
       "           0.1897,  0.0553,  0.0749,  0.1234,  0.1970, -0.0147, -0.1966, -0.0302,\n",
       "           0.0725, -0.1134, -0.0558, -0.0526,  0.0251,  0.1512, -0.0101,  0.1807,\n",
       "           0.0188,  0.0386,  0.1373, -0.1283,  0.1787,  0.0175, -0.0006,  0.0111,\n",
       "          -0.1304,  0.1691,  0.0882,  0.0559,  0.0856,  0.0267,  0.1167,  0.0038,\n",
       "           0.0235, -0.0503, -0.0140, -0.1049,  0.0123,  0.0445, -0.1738,  0.1865,\n",
       "           0.2511,  0.0524, -0.1550, -0.1011, -0.1356, -0.0417, -0.1074,  0.0111,\n",
       "          -0.0413, -0.0359,  0.2058,  0.1278, -0.0388,  0.1393,  0.2526, -0.0286,\n",
       "           0.0997,  0.0541,  0.0835,  0.0911, -0.0055,  0.2099, -0.0478,  0.1669,\n",
       "           0.2301,  0.0469,  0.1547,  0.2420,  0.2087,  0.0746,  0.1213,  0.1539,\n",
       "           0.2399,  0.0762,  0.0264,  0.0147,  0.0590,  0.1313,  0.1688, -0.1814,\n",
       "           0.1389,  0.1908,  0.2680, -0.0325,  0.2041,  0.2464,  0.0184,  0.2330,\n",
       "          -0.1356,  0.0284,  0.1577,  0.1820,  0.0480, -0.1920, -0.0863,  0.0859,\n",
       "          -0.0060,  0.2089, -0.1557,  0.2473,  0.1927,  0.0994,  0.2572,  0.0096],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0049, -0.0440,  0.0007,  ...,  0.2571, -0.0293,  0.0930],\n",
       "          [ 0.2388, -0.1971, -0.0489,  ...,  0.1873, -0.1239,  0.0424],\n",
       "          [ 0.0034, -0.0852, -0.1171,  ..., -0.0102, -0.1264,  0.2052],\n",
       "          ...,\n",
       "          [ 0.2286, -0.0328,  0.0295,  ...,  0.2065, -0.2398,  0.0712],\n",
       "          [ 0.0814, -0.2255, -0.1585,  ...,  0.1737, -0.1432,  0.2354],\n",
       "          [ 0.2007, -0.2782, -0.0283,  ...,  0.2200, -0.2278,  0.1139]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0752, -0.0760, -0.0039,  ...,  0.2259,  0.1180, -0.0655],\n",
       "          [-0.2219, -0.1176, -0.0255,  ...,  0.2841,  0.2108, -0.1381],\n",
       "          [-0.1242, -0.0677,  0.1040,  ...,  0.2760,  0.1703, -0.2020],\n",
       "          ...,\n",
       "          [-0.0186,  0.0182,  0.1795,  ...,  0.1598,  0.2383, -0.0762],\n",
       "          [-0.1850, -0.0295,  0.2548,  ...,  0.1225,  0.1858, -0.2722],\n",
       "          [-0.0566, -0.2301,  0.0648,  ...,  0.0235,  0.0701, -0.0806]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0973,  0.1968,  0.0635,  0.1127,  0.0039,  0.0711,  0.0916,  0.1378,\n",
       "           0.1693,  0.1068,  0.1172, -0.0013,  0.2127,  0.1987,  0.0127, -0.0159,\n",
       "           0.1031,  0.1206,  0.1579,  0.0229, -0.0400,  0.2153,  0.1080,  0.1016,\n",
       "           0.0540,  0.1149,  0.0058,  0.2005,  0.0749,  0.1601,  0.1324,  0.1795,\n",
       "           0.0244,  0.1250, -0.0119,  0.0122,  0.1956,  0.0455,  0.2003,  0.1450,\n",
       "           0.1523, -0.0388,  0.0907,  0.0981,  0.0584,  0.0027,  0.2101,  0.1775,\n",
       "           0.0726,  0.1694,  0.0420,  0.1174, -0.0485,  0.1164,  0.1037,  0.0896,\n",
       "          -0.0294,  0.0132, -0.0017,  0.0873,  0.1001,  0.1971, -0.1657,  0.2154,\n",
       "           0.0422,  0.1145,  0.2477,  0.1506,  0.1834,  0.1432,  0.0666,  0.1275,\n",
       "          -0.0313, -0.0662, -0.0623,  0.1396,  0.0170, -0.0022,  0.0394,  0.1321,\n",
       "           0.1574,  0.0620,  0.1993,  0.2352,  0.0401,  0.0847,  0.2227,  0.1245,\n",
       "          -0.0431, -0.1158,  0.0886, -0.1828,  0.1665,  0.0121, -0.0675,  0.0294,\n",
       "           0.0752,  0.1743,  0.2268,  0.0997, -0.2415, -0.0909,  0.0442,  0.0148,\n",
       "          -0.1140,  0.1218,  0.0444, -0.0364,  0.1269,  0.1946,  0.0228,  0.0259,\n",
       "           0.0856, -0.1637,  0.1873, -0.1692, -0.0134,  0.0317, -0.0435, -0.1066,\n",
       "           0.0192, -0.0581, -0.0571, -0.0046, -0.0507,  0.1269,  0.1335,  0.0964,\n",
       "           0.1965, -0.0378, -0.1922,  0.0266,  0.1725,  0.0168,  0.2367, -0.1492,\n",
       "          -0.0157,  0.2004,  0.0867,  0.0598,  0.0213, -0.2500,  0.0372, -0.0957,\n",
       "           0.0145, -0.0544,  0.0468, -0.1486,  0.1561, -0.0107,  0.0634,  0.0264,\n",
       "           0.1716,  0.2264,  0.0532,  0.2404,  0.0492,  0.0176,  0.2599,  0.0746,\n",
       "           0.2777,  0.1088,  0.0927,  0.0894,  0.1431,  0.0497,  0.1809,  0.2119,\n",
       "           0.1771,  0.0315,  0.0299, -0.0049,  0.0951,  0.0495,  0.0850,  0.0217,\n",
       "           0.1802,  0.1571,  0.0209, -0.0093,  0.2583,  0.0243,  0.2588,  0.0469,\n",
       "           0.1359,  0.0870,  0.0140,  0.2290, -0.0230,  0.0289, -0.0178,  0.0127,\n",
       "           0.2296,  0.0606,  0.2425,  0.0873,  0.1391,  0.0110,  0.0196,  0.0499],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 1.4148e-01,  6.4112e-02,  1.3397e-01,  3.8917e-02,  1.9726e-01,\n",
       "           1.7553e-01, -2.5901e-02,  1.1882e-01,  9.4273e-02,  1.2875e-01,\n",
       "          -4.1387e-02,  1.4934e-01,  1.1423e-01,  4.2752e-02,  1.9321e-01,\n",
       "           6.8174e-02,  1.9851e-01,  2.7101e-02,  1.4550e-01,  9.9837e-02,\n",
       "           9.2424e-03, -3.1303e-02,  2.1256e-01, -3.9353e-02,  4.7578e-02,\n",
       "          -2.0350e-02,  2.0374e-01,  2.2715e-01,  4.5302e-02,  3.8911e-02,\n",
       "           2.3845e-02,  1.7249e-01,  1.9597e-01,  6.0201e-02,  3.5817e-03,\n",
       "           1.5600e-02,  9.9471e-02,  4.0362e-02,  1.7777e-01,  6.9809e-02,\n",
       "          -1.8845e-02,  1.8240e-02,  8.2232e-02, -5.2591e-02,  2.8538e-02,\n",
       "           1.6056e-01,  7.3283e-03,  1.7552e-01,  1.2699e-01,  9.3991e-03,\n",
       "           1.1364e-01,  1.1813e-01,  8.6987e-02,  1.3181e-01,  1.5725e-01,\n",
       "           4.6506e-02,  1.5480e-01,  1.5132e-01,  1.7116e-01,  3.8355e-02,\n",
       "           2.7189e-02,  2.3081e-01, -9.5675e-02,  1.7317e-02,  1.5542e-01,\n",
       "           2.2253e-04,  1.7221e-01,  1.4799e-01,  1.7394e-01,  1.2854e-02,\n",
       "           1.7687e-02, -4.0848e-02, -5.7119e-02, -8.5432e-02, -6.1741e-02,\n",
       "           1.1412e-02,  2.5749e-01,  2.7487e-02, -6.1822e-02,  6.5673e-02,\n",
       "           4.1779e-02,  1.2303e-01,  8.5658e-02,  1.0381e-01,  3.8999e-02,\n",
       "           1.1542e-01,  1.2266e-01,  1.2628e-01,  6.3723e-02,  1.4932e-02,\n",
       "          -9.9980e-02, -1.4831e-03,  2.1505e-02,  1.8998e-01,  1.0812e-01,\n",
       "          -3.7616e-02,  1.9615e-01,  1.6894e-01,  1.1246e-01,  1.3311e-01,\n",
       "          -1.2336e-01, -1.4841e-01,  1.3935e-01, -6.1463e-02, -1.8650e-01,\n",
       "           4.9460e-02,  1.4958e-01, -2.2176e-01, -2.4094e-02,  6.1911e-02,\n",
       "           3.0197e-02,  1.5851e-01,  2.0635e-01, -1.1576e-02,  7.5326e-02,\n",
       "          -2.5639e-01,  2.3489e-01, -1.4476e-01, -1.4303e-01, -1.6459e-01,\n",
       "          -2.0728e-01, -1.9930e-01, -1.2514e-01,  3.5365e-02, -1.4200e-01,\n",
       "           3.0728e-02,  1.9805e-02,  2.4177e-01,  9.8005e-02,  1.1819e-02,\n",
       "           5.7816e-03,  2.1262e-01,  2.1218e-01,  1.2468e-01,  2.5587e-02,\n",
       "          -2.1182e-01,  2.3388e-01,  2.4451e-01,  4.6215e-02,  1.5966e-01,\n",
       "           3.4229e-02, -1.3055e-01, -3.0152e-03, -5.5213e-03,  8.8770e-02,\n",
       "          -2.1625e-04,  9.2436e-02,  8.1079e-02,  2.3138e-01, -1.8433e-01,\n",
       "           1.1280e-01,  9.6743e-02,  1.9016e-01,  9.3304e-02,  2.7465e-01,\n",
       "           3.2111e-02,  1.3741e-01,  2.3334e-01,  2.9929e-02,  2.5743e-01,\n",
       "           1.8150e-01,  1.5727e-01,  1.8197e-01,  6.6234e-02,  1.6788e-01,\n",
       "           9.1833e-02,  1.6464e-01,  2.6756e-01,  2.5769e-01,  1.4388e-01,\n",
       "           8.9348e-02,  1.6324e-02,  1.7829e-01,  2.2880e-02,  4.7027e-02,\n",
       "          -2.9961e-02,  1.8267e-01,  8.3989e-02,  9.1549e-02,  2.0434e-01,\n",
       "           5.8059e-03,  3.6421e-02,  1.5268e-01,  1.5318e-01,  2.1313e-01,\n",
       "           2.3353e-01,  2.4757e-01,  1.4638e-01,  2.4556e-01,  6.1259e-03,\n",
       "           4.3882e-02, -5.0362e-02,  7.1117e-02,  8.8528e-03,  1.0681e-01,\n",
       "           5.0285e-02,  8.3960e-02,  2.2086e-01,  1.8306e-01,  1.7205e-01],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0025, -0.1105,  0.0007, -0.0924, -0.2108,  0.0183,  0.1016, -0.2293,\n",
       "            0.1986,  0.0521,  0.1184,  0.1319, -0.1504, -0.1043,  0.0303,  0.1356,\n",
       "            0.1093, -0.2982, -0.2348, -0.1408,  0.1541, -0.1957, -0.0906, -0.2472,\n",
       "           -0.2524,  0.0178,  0.2699,  0.1255,  0.0759,  0.2223, -0.0181,  0.1394,\n",
       "            0.2919,  0.0355, -0.1080, -0.1000,  0.1727,  0.2829,  0.2277,  0.1115,\n",
       "           -0.1554,  0.1309, -0.1551,  0.0271, -0.6514,  0.1739,  0.1786,  0.0231,\n",
       "            0.3188, -0.1363],\n",
       "          [-0.0200, -0.1165,  0.1471, -0.2199, -0.1051,  0.2455,  0.1549, -0.1221,\n",
       "            0.1950,  0.1727,  0.1992,  0.1570, -0.0756, -0.2583, -0.0103, -0.1338,\n",
       "            0.2638, -0.2470, -0.2870, -0.1391,  0.0086, -0.2066, -0.0667, -0.2974,\n",
       "           -0.0921, -0.0389,  0.0554,  0.2741, -0.0017,  0.2170, -0.0364, -0.0217,\n",
       "            0.1003,  0.2320, -0.0114, -0.2295,  0.2817,  0.0451,  0.1252,  0.1647,\n",
       "           -0.1106, -0.0086, -0.1559, -0.1373, -0.4596,  0.1810,  0.1477, -0.0108,\n",
       "            0.2873, -0.1895],\n",
       "          [-0.5955, -0.5756,  0.4442, -0.5752, -0.5828,  0.4307,  0.4929, -0.5283,\n",
       "            0.5015,  0.4647,  0.5204,  0.5677,  0.4422, -0.4945,  0.6097, -0.5514,\n",
       "            0.5212, -0.5048, -0.5011, -0.5179, -0.4788, -0.5264, -0.5365,  0.5045,\n",
       "           -0.5581,  0.5694,  0.5415,  0.4929,  0.5439,  0.4449, -0.5343,  0.5719,\n",
       "            0.5516,  0.4488,  0.5045, -0.5954,  0.4758,  0.5983,  0.5077,  0.4826,\n",
       "           -0.4791, -0.5042, -0.4982, -0.6003, -0.3238,  0.4378,  0.6002,  0.4706,\n",
       "            0.5484, -0.5863]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0130,  0.2485,  0.4887], requires_grad=True)],\n",
       " 'lr': 0.006999999999999999,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Generator expression must be parenthesized (<ipython-input-67-d613366a3014>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-d613366a3014>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    loss_grads = grad(loss, p.numel() for p in seq.parameters() if p.requires_grad)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Generator expression must be parenthesized\n"
     ]
    }
   ],
   "source": [
    "loss_grads = grad(loss, optimizer.param_groups[0].items(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f1e7c7cf3f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, requires_grad=True, )\n",
    "y = torch.pow(x,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], grad_fn=<PowBackward0>)\n",
      "tensor(1., grad_fn=<MeanBackward0>)\n",
      "tensor([1.5000, 1.5000], grad_fn=<CopyBackwards>)\n",
      "out= tensor(1., grad_fn=<MeanBackward0>)\n",
      "tensor([3., 3.], grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "out = torch.mean(y)\n",
    "print(y)\n",
    "print(out)\n",
    "out.backward( retain_graph=True, create_graph=True)\n",
    "print(x.grad)\n",
    "print(\"out=\",out)\n",
    "out.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31553\n"
     ]
    }
   ],
   "source": [
    "print(n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tentativo maldestro di calcolare la FISHER MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (100, 1, 3)\n",
      "FILE:  0 STEP TEST:  0 test loss: 275.77957749551746 lr:  0.01 err:  27.477957749551745\n",
      "FILE:  0 STEP TEST:  10 test loss: 160.4649510896097 lr:  0.01 err:  11.531462640590775\n",
      "FILE:  0 STEP TEST:  20 test loss: 103.37890124004038 lr:  0.01 err:  5.708604984956933\n",
      "FILE:  0 STEP TEST:  30 test loss: 65.06304889480431 lr:  0.01 err:  3.8315852345236068\n",
      "FILE:  0 STEP TEST:  40 test loss: 45.56618361896786 lr:  0.01 err:  1.9496865275836448\n",
      "FILE:  0 STEP TEST:  50 test loss: 38.153735588798604 lr:  0.01 err:  0.7412448030169259\n",
      "FILE:  0 STEP TEST:  60 test loss: 36.49977916900028 lr:  0.01 err:  0.16539564197983267\n",
      "FILE:  0 STEP TEST:  70 test loss: 36.44273915356721 lr:  0.01 err:  0.005704001543306703\n",
      "FILE:  0 STEP TEST:  80 test loss: 36.47238650678617 lr:  0.01 err:  0.002964735321896228\n",
      "FILE:  0 STEP TEST:  90 test loss: 36.41053951547177 lr:  0.01 err:  0.006184699131440397\n",
      "FILE:  0 STEP TEST:  100 test loss: 36.347821676026484 lr:  0.01 err:  0.006271783944528408\n",
      "FILE:  0 STEP TEST:  110 test loss: 36.31298777445134 lr:  0.01 err:  0.0034833901575147762\n",
      "FILE:  0 STEP TEST:  120 test loss: 36.29025641902717 lr:  0.01 err:  0.002273135542416327\n",
      "FILE:  0 STEP TEST:  130 test loss: 36.267893871179936 lr:  0.01 err:  0.0022362547847237122\n",
      "FILE:  0 STEP TEST:  140 test loss: 36.2443844079176 lr:  0.01 err:  0.0023509463262335827\n",
      "FILE:  0 STEP TEST:  150 test loss: 36.22058124488574 lr:  0.01 err:  0.0023803163031857364\n",
      "FILE:  0 STEP TEST:  160 test loss: 36.1965985594537 lr:  0.01 err:  0.0023982685432045515\n",
      "FILE:  0 STEP TEST:  170 test loss: 36.17228327618219 lr:  0.01 err:  0.002431528327150545\n",
      "FILE:  0 STEP TEST:  180 test loss: 36.14762157461883 lr:  0.01 err:  0.0024661701563360337\n",
      "FILE:  0 STEP TEST:  190 test loss: 36.12266111425848 lr:  0.01 err:  0.0024960460360354377\n",
      "END: 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    # load data and make training set\n",
    "    hidden   = 50\n",
    "    layer    = 2\n",
    "    features = 3\n",
    "    dropout  = 0\n",
    "    # build the model\n",
    "    case = 'FULL'\n",
    "    #-----------LIONEL STUFF--------------#\n",
    "    n_params = sum(p.numel() for p in seq.parameters() if p.requires_grad)      # total nbr weights/biases\n",
    "    data = np.load(name)\n",
    "    data = np.expand_dims(data.T, axis=1)\n",
    "    len_train=data.shape[0]//1\n",
    "    sumgradexpcost = np.zeros(n_params)\n",
    "    sumexpcost     = np.zeros(n_params)        \n",
    "    cost = np.zeros(len_train-1)\n",
    "    gradcost = np.zeros((n_params, len_train-1))\n",
    "    \n",
    "    #-----------EO LIONEL STUFF-----------#\n",
    "    ini = 25\n",
    "    train_loss= []\n",
    "    train_loss_grad= []\n",
    "    for k in [0] :\n",
    "        name = './DATA/TRAIN_npy/'+case+'_'+str(k)+'.npy'\n",
    "        data = np.load(name)\n",
    "        data = np.expand_dims(data.T, axis=1)\n",
    "        data = data[:100] # reduce the number of time steps (db)\n",
    "        print(\"Data shape: \", data.shape)\n",
    "        input  = torch.from_numpy(data[:-1,:,:]).double().to(device)\n",
    "        target = torch.from_numpy(data[1:,:,:]).double().to(device)\n",
    "\n",
    "        seq = Sequence(hidden,layer,features,dropout).double().to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(seq.parameters(), lr =0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.7, patience=50, min_lr = 5e-5)\n",
    "        dirName = './RESULTS/'+case+'_'+str(k)\n",
    "        if not os.path.exists(dirName):\n",
    "            os.makedirs(dirName)\n",
    "        h_0 = torch.normal(mean=0.0, std=torch.ones(layer,input.size(1), hidden, dtype=torch.double)).to(device)\n",
    "        c_0 = torch.normal(mean=0.0, std=torch.ones(layer,input.size(1), hidden, dtype=torch.double)).to(device)\n",
    "\n",
    "        if k==0:\n",
    "            torch.save(seq.state_dict(),'./init_model.pt')\n",
    "        else:\n",
    "            state_dict = torch.load('./init_model.pt', map_location=device)\n",
    "            seq.load_state_dict(state_dict)\n",
    "\n",
    "        err = 10\n",
    "        loss1 = 1\n",
    "        i = 0\n",
    "        while loss1 > 1e-4 and i<200:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input,h_0,c_0)\n",
    "            loss = criterion(out, target.to(device))\n",
    "            loss.backward()\n",
    "            #-----------LIONEL STUFF--------------#\n",
    "            loss_np = loss.item() \n",
    "            gradcost_theta = np.array([])\n",
    "#            for param in seq.parameters():\n",
    "            for param in list(seq.parameters()):\n",
    "#                weight_or_bias = param.detach()           # extract from nn\n",
    "                weight_or_bias = param\n",
    "                w_or_b_grad = weight_or_bias.grad\n",
    "                w_or_b_grad = w_or_b_grad.view(-1).numpy()    # converts into np vector\n",
    "                gradcost_theta = np.append(gradcost_theta, w_or_b_grad)\n",
    "\n",
    "            cost[i] = loss.item()\n",
    "            gradcost[:,i] = gradcost_theta\n",
    "            exploss = np.exp(-loss_np)\n",
    "            sumgradexpcost += exploss * gradcost_theta\n",
    "            sumexpcost     += exploss\n",
    "            \n",
    "            \n",
    "            #-----------LIONEL STUFF--------------#\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss)\n",
    "            train_loss.append(loss.item())\n",
    "            #train_loss_grad.append()\n",
    "            torch.save(seq.state_dict(),dirName+'/mytraining.pt')\n",
    "            np.savetxt(dirName+'/loss.out',np.array([loss1]))\n",
    "            if (i%(10)==0):\n",
    "                loss2 = loss\n",
    "                err = np.abs(loss2.item()-loss1)/10\n",
    "                loss1 = loss2.item()\n",
    "                print('FILE: ', k,'STEP TEST: ', i, 'test loss:', loss2.item(), 'lr: ', optimizer.param_groups[0]['lr'], 'err: ', err)\n",
    "            i += 1\n",
    "            sys.stdout.flush() \n",
    "        print('END: '+str(k))\n",
    "        \n",
    "        normaliz = sumgradexpcost / sumexpcost\n",
    "        Jacob = np.zeros((n_params, n_params))\n",
    "        for irow in range(len_train-1):\n",
    "            gradlogLikelirow = -gradcost[:,irow] + normaliz\n",
    "            Jacob += np.outer(gradlogLikelirow, gradlogLikelirow)        # produces rank1 matrix Jacob            \n",
    "        \n",
    "#        InfoFisher = np.linalg.det(Jacob)\n",
    "        global lamb_Fisher            \n",
    "        lamb_Fisher = np.linalg.eigvals(Jacob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'h_t' and 'c_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7f81705326fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtargetloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtargetloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'h_t' and 'c_t'"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "        for i in range(len_train-1):\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input[i,:,:])\n",
    "            targetloc = target[i,:,:]\n",
    "            targetloc = targetloc[np.newaxis,:,:]\n",
    "            loss = criterion(out, targetloc)\n",
    "            loss_np = loss.item()\n",
    "            loss.backward()\n",
    "        \n",
    "            gradcost_theta = np.array([])\n",
    "#            for param in seq.parameters():\n",
    "            for param in list(seq.parameters()):\n",
    "#                weight_or_bias = param.detach()           # extract from nn\n",
    "                weight_or_bias = param\n",
    "                w_or_b_grad = weight_or_bias.grad\n",
    "                w_or_b_grad = w_or_b_grad.view(-1).numpy()    # converts into np vector\n",
    "                gradcost_theta = np.append(gradcost_theta, w_or_b_grad)\n",
    "\n",
    "            cost[i] = loss.item()\n",
    "            gradcost[:,i] = gradcost_theta\n",
    "            exploss = np.exp(-loss_np)\n",
    "            sumgradexpcost += exploss * gradcost_theta\n",
    "            sumexpcost     += exploss\n",
    "        \n",
    "#        normaliz = sumgradexpcost / np.log(sumexpcost)\n",
    "        normaliz = sumgradexpcost / sumexpcost\n",
    "        Jacob = np.zeros((n_params, n_params))\n",
    "        for irow in range(len_train-1):\n",
    "            gradlogLikelirow = -gradcost[:,irow] + normaliz\n",
    "            Jacob += np.outer(gradlogLikelirow, gradlogLikelirow)        # produces rank1 matrix Jacob            \n",
    "        \n",
    "#        InfoFisher = np.linalg.det(Jacob)\n",
    "        global lamb_Fisher            \n",
    "        lamb_Fisher = np.linalg.eigvals(Jacob)\n",
    "        # FIN de Fisher information matrix -- LM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
